---
title:  "[숩터디] 3주차 3"
excerpt: "Joonseok Lee 교수님의 Lecture 9. Attention Mechanism & Transformers I 강의 요약"

categories:
  - Blog
tags:
  - [Blog, Deeplearning, Github, SNU]

toc: true
toc_sticky: true
 
date: 2023-06-11
last_modified_at: 2023-06-11
---

# start
- 인코더가 기억해야 할 양이 많으면 과거의 정보가 잊혀지는 문제가 있다.
- 디코더에는 기억한 양만 전해주는 것도 문제임
- hidden state를 decoder에게 넘겨주는 아이디어에서 출발함

# Attention Idea
- Attention Function : Query, Key, Value를 입력으로 받고 Attention Value를 출력함
- Query를 기준으로 Key와 유사도를 계산하여 Value와 곱함
- Query / Key-Value : 하나의 단어 / 나머지 단어 또는 지금 출력해야 하는 순서 / 나머지에 해당하는 단어 같은 느낌
- Q, K, V, A는 대부분 같은 차원을 사용함
- Attention Score를 계산하고 Softmax를 통과시켜서 Attention 계수를 구하고 value에 곱하여 Attention value를 구함

# Transformer
- Query, Key, Value를 만들어서 사용함.
- 입력을 주변을 고려하여 새롭게 표현함
- Contextualize : 자신을 input으로 쓰는 경우 관련이 있는 변수들과의 관계를 생각하여 조점됨.
- 초기 모델은 순서에 상관없는 설계로 진행됨.
