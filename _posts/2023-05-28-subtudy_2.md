---
title:  "[숩터디] 1주차 2"
excerpt: "Joonseok Lee 교수님의 Lecture 2. Linear & Softmax Classifiers 강의 요약"

categories:
  - Blog
tags:
  - [Blog, Deeplearning, Github, SNU]

toc: true
toc_sticky: true
 
date: 2020-05-28
last_modified_at: 2023-05-28
---
# Parametric Approach
* 데이터를 함수에 넣어서 분류
* 데이터에 weight sum을 해서 score를 설정
* bias : 입력 데이터와 독립적으로 작용할 수 있도록 사용한다. 전체적인 흐름을 반영할 수 있음. bias를 묶어서 한번에 포함하여 계산할 수 있도록 행렬을 설계
* 클래스별로 weight table을 만들어 확인하므로 knn보다 효과적임
* 선형 분류의 경우 결과의 값을 해석하기 어려움. 확률처럼 해석할 수 있도록 계산할 필요가 있음. 이를 위해 활성화함수로 sigmoid function을 도입하고 class가 확장되면 softmax function을 이용

# How to find the Weights
* 무작위로 설정 – 데이터를 통해 학습(정답과 비교를 통해 업데이트)

# Loss function 
* 정답과 예측값을 곱하여 margin을 구할 수 있다. 
* margin의 부호를 통해 정답 여부, 값의 크기를 통해 예측을 얼마나 자신있게 했는지 이야기하는 것을 확인할 수 있음. 
* 양수인 부분에는 작은 loss나 loss를 주지 않고, 반대의 경우는 큰 loss를 준다. 
* 사례로는 log loss, exponential loss, hinge loss 등이 있음. 
* exponential loss는 outlier가 많은 경우에 좋지 않다. 나머지 둘이 많이 쓰인다. 

# cross entropy
* 정의는 따로 다시 식으로 정리(예정)
* 정답만 1이므로 정답인 클래스만 해서 시그마를 하나 삭제할 수 있음. 즉, 정답인 클래스만 신경쓰고 나머지에는 신경쓰지 않는 것이 특징임.

## KL Divergence
* 식은 따로 정리
* 분포가 같은지 비교하는 방식, 거리와 같지는 않으나 거리처럼 활용할 수 있음.
* 교수님이 깊게 알 필요는 없다 했는데 그래도 알아보자

# optimization
* 최소화하고 싶은 함수(loss, cost 등)를 최소화하여 최적화를 하는 과정
* gradient descent를 이용하는데 항상 수렴할 것이라 보장할 수 없고 모든 경우에 대해 구하면 느리다. 
* 후자를 해결하기 위해 sampling(minibatch)을 함.

### 학습의 종료 시점은 data와 model에 따라 다르기 때문에 적절히 정해야함. 

### 사이토 고키의 밑바닥부터 시작하는 딥러닝 내용을 같이 정리하면 깔끔하게 정리할 수 있을 듯
