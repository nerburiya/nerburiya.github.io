---
title:  "[숩터디] 3주차 2"
excerpt: "Joonseok Lee 교수님의 Lecture 8. Recurrent Neural Networks II (LSTMs & Seq2seq Models) 강의 요약"

categories:
  - Blog
tags:
  - [Blog, Deeplearning, Github, SNU]

toc: true
toc_sticky: true
 
date: 2023-06-11
last_modified_at: 2023-06-11
---

# Exploding / Vanishing Gradient Problem
- 역전파에서 tanh의 미분값이 극단에서 매우 작기에 Vanishing 문제가 발생함
- 똑같은 가중치를 사용하는 것이 역전파 과정에서 가중치 행렬을 반복해서 곱하게 만들고 이는 1보다 작은 값은 Vanishing, 1보다 큰 값은 Exploding하게 만든다.
- Exploding하는 경우 Clipping을 사용하여 해결 가능(벡터의 방향은 유지하고 크기는 줄인다.)

# Towards Modeling Longer Dependence
## LSTM (long short term memory)
- Cell state를 만들어서 장기기억을 보존
- forget gate를 만들어서 장기기억 보존 여부를 보정
- 새로운 입력을 장기기억으로 반영할 수 있는 input gate
- 장기기억이 단기기억으로 전달될 수 있는 output gate
- 각각의 gate는 sigmoid를 통해서 얼마나 사용할지 score를 정한다.
- tanh를 통과하는 것이 데이터의 흐름임
- Vanishing Gradient Problem 일부 해결, 장기기억 일부 보존 가능
- 입력의 차원, hidden state의 차원, number of layer, 단어의 길이, 배치 사이즈까지 RNN과 같으나 초기화할 값과 output의 개수만 다름
- 일반적을 RNN을 잘 안쓰고 LSTM을 쓴다. 근데 Transformer를 결국 쓰게 됨.

## GRU(Gate Tecurrent Units)
- convex combination을 이용(내분점 같은 느낌), input과 forget이 관계가 있다고 판단하였기 때문
- Cell state가 따로 없이 게이트가 하나 적어서 LSTM보다 가벼움

# Sequence-to-Sequence(seq2seq) Model
- input 길이와 output 길이가 다른 경우
- RNN은 1-1 관계를 가정하고 있음, 번역과 같은 경우에 적합하지 않음
- encoder : 일단 문장을 전부 받아서 기억
- decoder : encoder에서 기억한 벡터를 받아오고 시작에 해당하는 명령어를 주면 output을 만들어낸다. (다음 input에 기억한 문장 벡터와 이전에 출력한 output)
- Auto_Regressive : 한 단계마다 output을 출력하고 그걸 다음 input으로 사용하는 것
- Teacher Forcing : 학습할 때는 다음에 들어가는 것을 Auto_Regressive에서 output을 넘기는 것이 아니라 정답을 넘겨줌. 안 그러면 틀린 입력에 대해서 학습하는데 그러면 의미가 없음
