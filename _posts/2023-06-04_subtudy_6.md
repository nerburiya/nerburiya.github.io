---
title:  "[숩터디] 2주차 3"
excerpt: "Joonseok Lee 교수님의 Lecture 6. Training Neural Networks II 강의 요약"

categories:
  - Blog
tags:
  - [Blog, Deeplearning, Github, SNU]

toc: true
toc_sticky: true
 
date: 2023-06-04
last_modified_at: 2023-06-04
---

# Optimization
- SGD의 문제 : Jittering, Local optimum, Saddle point, Inaccurate Gradient Estimation(mini-batch가 전체를 대변하지 못함)
- SGD + Momentum(실제로는 관성에 더 가까움) : velocity 항을 만들어서 추가함
- AdaGrad : 이동한 방향에 대한 누적값을 저장(덜 간 방향으로 더 많이 가도록 설정) but 많이 간 방향에 대한 이동이 사라질 위험이 있음. 이 문제를 개선하고자 RMSProp이 나옴
- Adam : RMSProp과 SGD + Momentum의 결합
- First vs Second order optimization : 2차 테일러 전개를 하면 좋은데 해시안 행렬이 너무 크다.
- Adam을 많이 쓴다. 초기 학습률을 선정하는 것이 중요하고 learning schedule을 잘 조정하면 Adam보다 SGD + Momentum이 더 좋은 경우도 있다.

# Regularization
##  머신러닝
- additional penalty term
- force the weight to be small
## 딥러닝
- weight decay : 머신러닝에서 L1, L2와 같이 penalty term을 부여
- Early stopping : validation set의 loss가 커지는 시점
- Drop out : 랜덤하게 일부 노드를 작동하지 않도록 설정(특정 피처에 over fitting 하는 것을 방지할 수 있음), test 할 때도 drop out 확률을 곱해주거나 train에서 확률로 나눠줘야 함. (스케일이 달라지는 것을 방지하기 위함)
- Cut out : 입력 이미지의 일부를 잘라내는 것(자르고 검게 채움)
- 실제로는 Drop out, Early stopping(가급적 최종 지표를 가지고 측정해야 함), data augmentation과 Batch Normalization을 사용함.

# Batch Norm
- 다음 층으로 갈 때 데이터가 normalized 되어 있다는 보장을 할 수 없음. 이를 해결하고 싶어서 층마다 input을 Normalization을 해줌.
- batch의 평균과 표준편차를 이용하여 계산함. 
- 마음대로 Normalization을 하면 train에 문제가 발생하므로 값을 회복시킬 수 있는 방법이 필요함. 선형 결합을 통해 회복시킬 수 있도록 만듦. (표준편차를 곱하고 평균을 더하는 항을 만들어 각각의 항을 학습하도록 설정함)
- test에서는 train에서 사용했던 평균과 표준편차를 사용한다.
- Normalization의 장점을 매 층에서 얻을 수 있음.
- 하지만 분포의 동일성을 가정할 수 없으므로 한계가 있음.
- FC-BN-Activation-FC-BN-Activation 형태로 쌓아간다.
- Layer Normalization, Instance Normalization, Group Normalization 등도 있음
