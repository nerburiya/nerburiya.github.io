---
title:  "[숩터디] 2주차 2"
excerpt: "Joonseok Lee 교수님의 Lecture 5. Training Neural Networks I 강의 요약"

categories:
  - Blog
tags:
  - [Blog, Deeplearning, Github, SNU]

toc: true
toc_sticky: true
 
date: 2020-06-04
last_modified_at: 2023-06-04
---

# Activation Function
- non-linearity : sigmoid, tanh, relu, Leaky ReLU, ELU 
- sigmoid 단점 : killed gradients, Not Zero-centered Output(입력이 모두 양수일 때 문제, 모든 그래디언트가 부호가 바뀌지 않음, 효율이 떨어짐), e의 계산이 오래걸림
- tanh의 단점 : killed gradients
- relu 단점 : Not Zero-centered Output, 0에서 미분 불가, 초기 값이 음수면 문제가 있음

# Data Preprocessing 
- Zero-ceterimg, Normalization
- PCA, Whitening(PCA+Normalization)

# Data Augmentation
- shift, box, tint 등의 변화를 주는 것. 데이터 증강(다양성 추가)
- 좌우 반전, 상하 반전, random crops, scaling(사이즈 변환, reasonable size로 진행)
- color jitter(색조에 변화를 준다) : hue(색조), saturation(채도), lightness(명도)의 세 채널로 색을 하나씩 대응할 수 있음 (rgb와 hsl은 서로 변환이 가능)
- 그 외에도 다양한 방식이 존재함

# Weight Initialization
- Small Gaussian Random : 가우시안 분포(정규분포)를 따르고 분산(0.01)을 아주 작게 만든다. tanh를 활성화 함수로 쓰게 되면 분산이 점점 줄어들어서 한 값에 몰리는 현상이 발생함, 특히 표준정규분포면 0으로 몰림
- Large Gaussian Random : 가우시안 분포(정규분포)를 따르고 분산(0.5)을 조금 크게 만든다. tanh를 활성화 함수로 쓰게 되면 값이 양극단으로 수렴해버리는 문제가 발생
- Xavier Initialization : 초깃값을 가우시안 분포(정규분포)를 따르는 랜덤한 값을 input의 차원의 제곱근으로 나눠준다. conv에서 input의 차원=F^2 * C(F는 필터 사이즈, C는 채널의 수). [교수님의 공식 추후 추가] 층을 통과해도 분산이 크게 변하지 않을 수 있음.
- Kaiming/MARA Initialization for ReLU : input의 차원의 제곱근의 절반으로 나눔

# Learning Rate Scheduling
- Learning Rate Decay : 수렴할 때쯤 Learning Rate를 줄여나감. ex) 반정도 진행할 때마다 1/10로 줄임, cosine, linear, inverse sqrt 등의 방법도 있음. 
- Learning Rate Decay – Initial Warmup : 처음에 크게 시작하면 발산할 수 있으므로 초반에는 천천히 Learning Rate를 증가시키면 잘 된다.
- 시행착오를 거쳐서 찾을 수밖에 없다.
