---
title:  "쪼랩 데이터 분석 스터디"
excerpt: "Chapter 10, 다시 살펴보는 딥러닝 주요 개념"

categories:
  - Blog
tags:
  - [Blog, Deeplearning, Github, SNU, ZZOLAB]

toc: true
toc_sticky: true
 
date: 2023-10-14
last_modified_at: 2023-10-14
---

# 10.1 인공신경망
- 인공진경망은 인간의 생물학적 신경망과 유사한 구조를 갖는 인공 시스템
  
## 10.1.1 퍼셉트론
- 뉴런의 원리를 본떠 만든 인공 구조
- 입력 값에 가중치를 곱하여 합한 값을 계산
- 위 계산 값에 활성화 함수를 적용하여 출력값을 설정

## 10.1.2 신경망
- 입력층, 은닉층, 출력층으로 구성

## 10.1.3 활성화 함수
- sigmoid
- ReLU(기본, Leaky, ELU(Exponential linear Unit))
- tanh
- maxout
[참고](https://heeya-stupidbutstudying.tistory.com/entry/ML-%ED%99%9C%EC%84%B1%ED%99%94-%ED%95%A8%EC%88%98Activation-Function)

## 10.1.4. 경사하강법
- 손실함수 : 모델 성능이 얼마나 나쁜지 측정하는 함수. 평균 제곱오차, 교차 엔트로피 등이 있음
- 경사하강법 : 기울기를 이용하여 가중치를 갱신하는 방법.
- 갱신할 가중치 = 기존 가중치 - (학습률) * (기울기)
- 확률적 경사하강법(SGD) : 전체 학습 데이터에서 개별 데이터를 무작위로 뽑아 경사 하강법을 수행하는 알고리즘
- 미니배치 경사하강법 : SGD는 효율이 떨어짐으로 미니배치 단위로 경사하강법을 시행

## 10.1.5 순전파와 역전파
- 순전파 : 신경망에서 입력값이 입력층과 은닉층을 거쳐 출력층에 도달하기까지의 계산 과정
- 역전파 : 순전파의 반대 개념
- 순전파로 오차를 계산하고 계산된 오차를 역전파를 이용하여 가중치를 갱신함

# 10.2 합성곱 신경망(CNN)
- 컴퓨터 비전 분야에서 주로 쓰이는 신경망
- 합성곱 계층, 풀링 계층, 전결합 계층으로 구성

## 10.2.1 합성곱 계층
- 합성곱 : 2차원 데이터의 일정 영역 내의 값들을 하나의 값으로 압축하는 연산
- 필터 : 입력 데이터에서 특정한 특성을 필터링하는 역할
  [참고](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.sobel.html)
- 특성 맵(피처 맵) : 합성곱 연산으로 얻은 결과
- 필터는 여러장을 사용하게 되고 그런 경우에 출력은 2차원 평면이 아닌 공간형태의 텐서를 얻을 수 있음 

## 10.2.2 패딩과 스트라이드
- 패딩 : 입력 데이터 주변을 특정 값으로 채우는 것
- 출력되는 특성맵과 입력 데이터의 크기를 동일하게 유지하기 위한 방법
- 스트라이드 : 필터가 한 번에 이동하는 간격, 스트라이드가 클 수록 특성 맵 크기는 작아짐
- 
### 출력데이터의 크기(중요)
- 출력 데이터의 크기 $$N_o$$, 입력 데이터의 크기 $$N_i$$, 패딩 $$P$$, 스트라이트 $$S$$, 필터의 크기 $$K$$
- $$N_o$$ = $$\left[ \frac{N_i + 2 P -K }{S} \right] +1$$

## 10.2.3 풀링
- 풀링 : 특성 맵 크기를 줄여서 이미지의 요약 정보를 추출하는 기능
- 특성 맵 크기를 줄여 연산 속도를 빠르게 하고 이미지에서 물체의 위치가 바뀌어도 같은 물체로 인식하게 만들기 위함(위치 불변성)
- 최대 풀링, 평균 풀링 등이 있음
- 일반적으로 풀링 커널 크기와 스트라이드 크기를 같게 하고 패딩 크기를 0으로 한다.
- 풀링 후 출력 데이터의 크기 $N_o$, 풀링 전 입력 데이터의 크기 $$N_i$$, 풀링 커널 크기(스트라이드 크기) $$K$$라 하면
- $$N_o$$ = $$\left[ \frac{N_i}{K} \right]$$
- $$N_o = \left[ \frac{N_i + 2P - K}{S} \right] + 1$$

## 10.2.4 전결합
- 전결합 : 이전 계층의 모든 노드 각각이 다음 계층의 노드 전부와 연결된 결합
- 전결합 계층 : 전결합으로 구성된 계층, 밀집계층이라고도 함.
- CNN에서는 보통 마지막 부분에서 구현한다.
- 다차원 데이터를 1차원으로 바꾸는 평탄화 작업 후 전결합이 진행됨

## 10.2.5 합성곱 신경망 전체 구조
- 합성곱 계층 - 풀링계층 - 평탄화 - 전결합 계층 - 출력

# 10.3 성능 향상을 위한 딥러닝 알고리즘
## 10.3.1. 드롭아웃
- 과대적합을 방지하기 위해 신경망 훈련 과정에서 무작위로 일부 뉴런을 제외하는 기법
- 매 훈련 이터레이션마다 새롭게 적용됨
- 훈련 단계에서만 적용함

## 10.3.2 배치 정규화
- 과대적합 방지와 훈련 속도 향상을 위한 기법
- 내부 공변량 변화 현상을 해결하기 위한 기법
- 내부 공변량 변화 : 신경망 계층 마다 입력 데이터 분포가 다른 현상(데이터 분포의 편차가 다르다)
- 정규화 : 데이터가 정규분포로 변환하는 작업
- 미니 배치를 정규화하고 스케일 조정 후 이동

### 스케일 조정 및 이동 
- 정규화한 데이터의 스케일을 조정하고 이동
- 정규화를 하면 값이 대부분 0 근처로 모이는데, 활성화 함수 중 시그모이드는 0 근처에서 선형성을 가지게 되어 문제가 발생함
- 활성화 함수는 비선형성을 주기 위해 사용하는 것임을 잊지 말기

## 10.3.3 옵티마이저
- 신경망의 최적 가중치를 찾아주는 알고리즘
- 모멘텀, Adagrad, RMSprop 등이 **있었음**
### Adam
- 거의 얘만 쓴다고 봐도 무방함

## 10.3.4. 전이 학습
- 전이 학습 : 사전 훈련된 모델을 사용하는 방법으로 아래 두 가지가 있음
- 전체 파라미터를 갱신하는 파인튜닝 방법
- 전결합 계층의 파라미터만 갱신하는 방법
