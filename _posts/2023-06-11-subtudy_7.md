---
title:  "[숩터디] 3주차 1"
excerpt: "Joonseok Lee 교수님의 Lecture 7. Sequential Data & Recurrent Neural Networks I 강의 요약"

categories:
  - Blog
tags:
  - [Blog, Deeplearning, Github, SNU]

toc: true
toc_sticky: true
 
date: 2023-06-11
last_modified_at: 2023-06-11
---

# Sequential Data
- 순서가 관계가 있고 중요함.
- 영상, 언어, 주식, 기상 상황 등
- 이전 시점까지의 데이터를 가지고 다음 시점을 예측
- 이미지를 설명하는 텍스트를 만드는 경우 입력은 시계열이 아닌데 출력은 시계열임.
- one-to-one, many-to-one, many-to-many, one-to-many

# word embedding
- word : 의미를 가진 최소의 단위
- 일상에서는 다음 단어를 고려해서 말하는 것이 아니라 자연스럽게 나온다
- 단어도 벡터로 표현, 의미가 유사할수록 가깝게
- 같이 사용되는 많은 단어를 가지고 학습하여 벡터화함
- word2vec 주변 단어와 현재 단어의 확률분포를 이용하여 예측(최대우도추정법 사용)
- GloVe : Global vector, 두 단어와 등장하는 확률을 각각 구한 후에 비율을 이용하여 모델링함. 선형적 연결이 가능한 것을 발견함.

# Recurrent Neural Networks(Sequential Classification(many-to-one))
- sentiment analysis problem
- CNN을 적용하기에는 입력의 크기가 일정하지 않음. 
- 새로운 아이디어 : Encoder(입력의 크기에 상관없이 할 수 있는)
- RNN은 internal state를 가지고 있어서 지금까지 입력된 내용을 기억함
- 입력이 들어올 때마다 업데이트를 진행함. 업데이트에서는 같은 함수로 진행함. 입력과 이전에 기억한 내용에 각각의 가중치를 곱하고 활성화함수(tanh)를 통과시켜서 업데이트 함. 즉 매 스테이지에서 가중치는 공유됨.(같은 가중치를 사용함)

# Recurrent Neural Networks(Sequential prediction(many-to-many))
- language : 단어가 규칙에 의해 나열된 것
- n-grams(딥러닝 이전 모델) : 각 문장에서 첫 단어- 다음 단어 – 순서로 확률을 계산해서 문장이 등장할 확률을 계산함. 모든 문장을 할 수는 없으니 앞, 뒤의 문장만으로 판단(n은 판단에 사용하는 문자의 개수)
- 문장이 있을 확률을 계산할 수 없음. 문장 단위의 학습이 안 되고, 단어 단위로 진행하게 됨.
- RNN도 multi-layer가 가능하다.
- PyTorch에서 API를 제공함(vanilla RNN), 입력의 차원보다 hidden state의 차원이 높도록 설정한다. 문장의 최대 길이를 설정하는 것은 처리의 효율성을 위해서 세팅함.

# RNN Trade-off
- 장점 : 입력의 크기에 상관없이 처리가 가능함. 더 큰 입력이 들어와도 모델이 커지지 않음. 이전 내용을 기억하고 있음. 같은 가중치를 사용함
- 단점 : 느리다(순서대로 해야 하기 때문에 병렬화가 어려움), vanishing gradient, 장기기억이 어려움. 
